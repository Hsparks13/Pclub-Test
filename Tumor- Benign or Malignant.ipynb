{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cf9b30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6346e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tumor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "04d94cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Sample code number'], axis=1, inplace = True)\n",
    "# Lets trim the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "53921058",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Class = [1 if each == 4 else 0 for each in data.Class] \n",
    "# This is for easier interpretation of data... 4 represented a malignant tumor while 2 a benign tumor, I changed the values to 1 and 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "68d1e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Class.values\n",
    "x = data.drop(['Class'], axis=1)\n",
    "#Segregating the database characters and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "74c01d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "C:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "x = (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d20a0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "# I would use 20% of the dataset for test purpose and 80% as the training set.\n",
    "\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "39085e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight_bias(dimension):\n",
    "    w = np.full((dimension,1), 0.01) \n",
    "    b = 0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a03e0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y_head = 1 / (1 + np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "148f4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w, b, x_train, y_train):\n",
    "   \n",
    "    z = np.dot(w.T, x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    \n",
    "    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)     \n",
    "    cost = (np.sum(loss)) / x_train.shape[1]                               \n",
    "    \n",
    "   \n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    \n",
    "    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}\n",
    "    \n",
    "    return cost, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "972e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w, b, x_train, y_train, learning_rate, iteration):\n",
    "    cost_list = []\n",
    "      \n",
    "    for i in range(iteration):\n",
    "        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n",
    "        cost_list.append(cost)\n",
    "    \n",
    "        # Updating weight and bias values:\n",
    "        w = w - learning_rate * gradients['derivative_weight']\n",
    "        b = b - learning_rate * gradients['derivative_bias']\n",
    "    \n",
    "    parameters = {'weight': w, 'bias':b}\n",
    "    \n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "054f233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, b, x_test):\n",
    "    z = sigmoid(np.dot(w.T, x_test) + b)\n",
    "    y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    \n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            y_prediction[0,i] = 0\n",
    "        else:\n",
    "            y_prediction[0,i] = 1\n",
    "            \n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a13c4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, iteration):\n",
    "    dimension = x_train.shape[0]\n",
    "    w, b = initialize_weight_bias(dimension)   \n",
    "    \n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, iteration)\n",
    "    \n",
    "    # Lets use x_test for predicting y:\n",
    "    y_test_predictions = prediction(parameters['weight'], parameters['bias'], x_test) \n",
    "    \n",
    "    print('Test accuracy: {}%'.format(100 - np.mean(np.abs(y_test_predictions - y_test))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "83f4c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.08029197080292%\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, iteration=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f844342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a51bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3121575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
